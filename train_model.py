"""
Script d'entra√Ænement modulaire pour un mod√®le sp√©cifique
Usage: python train_model.py --model logistic
Options: logistic, random_forest, gradient_boosting, svm
"""

import pandas as pd
import numpy as np
import argparse
import joblib
import time
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
from scipy.stats import randint, uniform
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBClassifier

# ============================================================================
# CONFIGURATION
# ============================================================================
RANDOM_STATE = 42
TEST_SIZE = 0.2
CV_FOLDS = 5
N_ITER = 50  # Nombre d'it√©rations pour RandomSearch

# PCA : mettre None pour d√©sactiver, ou un nombre (ex: 50, 100, 200)
USE_PCA = None 
# Si USE_PCA = None, le script utilisera toutes les dimensions

# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================
def normalize_title(title):
    """Normalise les titres de poste"""
    title_lower = str(title).lower()
    
    if 'data analyst' in title_lower or 'business analyst' in title_lower:
        return 'Data Analyst'
    elif 'data engineer' in title_lower:
        return 'Data Engineer'
    elif 'data scientist' in title_lower or 'machine learning' in title_lower:
        return 'Data Scientist'
    elif 'analyst' in title_lower:
        return 'Data Analyst'
    elif 'engineer' in title_lower:
        return 'Data Engineer'
    elif 'scientist' in title_lower:
        return 'Data Scientist'
    else:
        return 'Other'

def load_and_prepare_data():
    """Charge et pr√©pare les donn√©es"""
    print("\n" + "="*70)
    print(" CHARGEMENT ET PR√âPARATION DES DONN√âES")
    print("="*70)
    
    # Chargement
    df = pd.read_csv("BD_nettoy√©e.csv")
    X = np.load('skills_embeddings_all.npy')
    
    print(f"‚úì Dimensions des embeddings : {X.shape}")
    
    # Normalisation des titres
    df['title_normalized'] = df['title'].apply(normalize_title)
    y = df['title_normalized'].values
    
    # Filtrage des classes rares
    counts = df['title_normalized'].value_counts()
    common_titles = counts[counts >= 15].index
    mask = df['title_normalized'].isin(common_titles)
    
    X = X[mask]
    y = y[mask]

    # Supprimer la classe "Other" (trop peu d'exemples et peu coh√©rente)
    mask_no_other = y != 'Other'
    X = X[mask_no_other]
    y = y[mask_no_other]
    print(f"‚úì Classe 'Other' supprim√©e : {len(y)} exemples restants")
    
    print(f"‚úì Classes conserv√©es : {list(common_titles)}")
    print(f"‚úì Nombre d'exemples : {len(y)}")
    
    # Encodage
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    
    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_encoded
    )
    
    print(f"‚úì Train set : {len(X_train)} exemples")
    print(f"‚úì Test set  : {len(X_test)} exemples")
    
    return X_train, X_test, y_train, y_test, le

def create_preprocessing_pipeline(use_pca=None):
    """Cr√©e le pipeline de pr√©traitement (StandardScaler + PCA optionnel)"""
    steps = [('scaler', StandardScaler())]
    
    if use_pca is not None:
        steps.append(('pca', PCA(n_components=use_pca, random_state=RANDOM_STATE)))
        print(f"‚úì PCA activ√© : r√©duction √† {use_pca} dimensions")
    else:
        print(f"‚úì PCA d√©sactiv√© : utilisation de toutes les dimensions")
    
    return Pipeline(steps)

# ============================================================================
# CONFIGURATION DES MOD√àLES
# ============================================================================
MODEL_CONFIGS = {
    'logistic': {
        'name': 'Logistic Regression',
        'model': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),
        'params': {
            'model__C': [0.001, 0.01, 0.1, 1, 10, 100],
            'model__penalty': ['l2', None],
            'model__solver': ['lbfgs', 'saga'],
            'model__class_weight': ['balanced', None]
        }
    },
    'random_forest': {
        'name': 'Random Forest',
        'model': RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),
        'params': {
            'model__n_estimators': randint(100, 500),
            'model__max_depth': [10, 15, 20, 30, None],
            'model__min_samples_split': randint(2, 20),
            'model__min_samples_leaf': randint(1, 8),
            'model__max_features': ['sqrt', 'log2', None],
            'model__class_weight': ['balanced', None]
        }
    },
    'gradient_boosting': {
        'name': 'Gradient Boosting',
        'model': GradientBoostingClassifier(random_state=RANDOM_STATE),
        'params': {
            'model__n_estimators': randint(100, 400),
            'model__learning_rate': uniform(0.01, 0.19),
            'model__max_depth': randint(3, 15),
            'model__min_samples_split': randint(2, 20),
            'model__min_samples_leaf': randint(1, 8),
            'model__subsample': uniform(0.7, 0.3),
            'model__max_features': ['sqrt', 'log2', None]
        }
    },
    'xgboost': {
    'name': 'XGBoost',
    'model': XGBClassifier(
        random_state=RANDOM_STATE,
        n_jobs=-1,              # utilisation multithread CPU
        use_label_encoder=False,
        eval_metric='mlogloss'  # √©vite les warnings
    ),
    'params': {
        'model__n_estimators': randint(100, 400),
        'model__learning_rate': uniform(0.01, 0.3),
        'model__max_depth': randint(3, 15),
        'model__subsample': uniform(0.7, 0.3),
        'model__colsample_bytree': uniform(0.7, 0.3),
        'model__gamma': uniform(0, 0.5),
        'model__min_child_weight': randint(1, 8)
    }
}

}

# ============================================================================
# FONCTION D'ENTRA√éNEMENT
# ============================================================================
def train_model(model_key, X_train, X_test, y_train, y_test, le):
    """Entra√Æne un mod√®le sp√©cifique avec optimisation des hyperparam√®tres"""
    
    config = MODEL_CONFIGS[model_key]
    model_name = config['name']
    
    print("\n" + "="*70)
    print(f" ENTRA√éNEMENT : {model_name}")
    print("="*70)
    
    start_time = time.time()
    
    # Cr√©er le pipeline complet (pr√©traitement + mod√®le)
    preprocessing = create_preprocessing_pipeline(use_pca=USE_PCA)
    full_pipeline = Pipeline([
        ('preprocessing', preprocessing),
        ('model', config['model'])
    ])
    
    # Random Search
    print(f"\nüîç Recherche des meilleurs hyperparam√®tres...")
    print(f"   - It√©rations : {N_ITER}")
    print(f"   - Cross-validation : {CV_FOLDS} folds")
    
    random_search = RandomizedSearchCV(
        estimator=full_pipeline,
        param_distributions=config['params'],
        n_iter=N_ITER,
        cv=CV_FOLDS,
        scoring='accuracy',
        n_jobs=-1,
        random_state=RANDOM_STATE,
        verbose=1
    )
    
    # Entra√Ænement
    random_search.fit(X_train, y_train)
    
    # Meilleurs param√®tres
    print(f"\n‚úì Meilleurs param√®tres trouv√©s :")
    for param, value in random_search.best_params_.items():
        print(f"   {param}: {value}")
    
    # √âvaluation
    best_pipeline = random_search.best_estimator_
    cv_score = random_search.best_score_
    y_pred = best_pipeline.predict(X_test)
    test_score = accuracy_score(y_test, y_pred)
    
    training_time = time.time() - start_time
    
    print(f"\n R√âSULTATS :")
    print(f"   Score CV (train) : {cv_score:.4f}")
    print(f"   Score Test       : {test_score:.4f}")
    print(f"   Temps d'entra√Ænement : {training_time/60:.2f} minutes")
    
    # Rapport de classification
    print(f"\n Rapport de classification :")
    print(classification_report(
        y_test, y_pred,
        labels=np.unique(y_test),
        target_names=le.inverse_transform(np.unique(y_test)),
        zero_division=0
    ))
    
    # Matrice de confusion
    cm = confusion_matrix(y_test, y_pred)
    class_names = le.inverse_transform(np.unique(y_test))
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Nombre de pr√©dictions'})
    plt.title(f'Matrice de Confusion - {model_name}', fontsize=14, fontweight='bold')
    plt.ylabel('Vraie classe', fontsize=12)
    plt.xlabel('Classe pr√©dite', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    
    confusion_file = f'confusion_matrix_{model_key}.png'
    plt.savefig(confusion_file, dpi=300, bbox_inches='tight')
    print(f"\n‚úì Matrice sauvegard√©e : {confusion_file}")
    plt.close()
    
    # Sauvegarde du mod√®le
    model_file = f'model_{model_key}.pkl'
    results_file = f'results_{model_key}.pkl'
    
    joblib.dump(best_pipeline, model_file)
    
    results = {
        'model_name': model_name,
        'cv_score': cv_score,
        'test_score': test_score,
        'training_time': training_time,
        'best_params': random_search.best_params_,
        'confusion_matrix': cm,
        'class_names': class_names
    }
    joblib.dump(results, results_file)
    
    print(f"‚úì Mod√®le sauvegard√© : {model_file}")
    print(f"‚úì R√©sultats sauvegard√©s : {results_file}")
    
    return best_pipeline, results

# ============================================================================
# MAIN
# ============================================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Entra√Æner un mod√®le sp√©cifique')
    parser.add_argument('--model', type=str, required=True,
                      choices=['logistic', 'random_forest', 'gradient_boosting', 'xgboost'],
                      help='Mod√®le √† entra√Æner')
    
    args = parser.parse_args()
    
    print("\n" + "üöÄ " + "="*66 + " üöÄ")
    print(f"   ENTRA√éNEMENT DU MOD√àLE : {MODEL_CONFIGS[args.model]['name']}")
    print("üöÄ " + "="*66 + " üöÄ")
    
    # Chargement des donn√©es
    X_train, X_test, y_train, y_test, le = load_and_prepare_data()
    
    # Sauvegarde du label encoder (une seule fois)
    joblib.dump(le, 'label_encoder.pkl')
    print("\n‚úì Label encoder sauvegard√© : label_encoder.pkl")
    
    # Entra√Ænement
    model, results = train_model(args.model, X_train, X_test, y_train, y_test, le)
    
    print("\n" + "="*70)
    print(" ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS !")
    print("="*70)
    print(f"\n  Fichiers g√©n√©r√©s :")
    print(f"   ‚Ä¢ model_{args.model}.pkl")
    print(f"   ‚Ä¢ results_{args.model}.pkl")
    print(f"   ‚Ä¢ confusion_matrix_{args.model}.png")
    print(f"   ‚Ä¢ label_encoder.pkl")
    print(f"\n  Accuracy finale : {results['test_score']:.4f}")
    print("="*70 + "\n")